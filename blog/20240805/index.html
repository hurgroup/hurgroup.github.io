<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/favicon.png">

   <title>[2024/8/5] Crash Course on Decision Process (I): MDP</title>  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1><a href="/">HUR Group</a></h1>
      <p class="lead">Dynamics and Biomechatronics Lab</p>
    </div>
    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">Home</a>
      <a class="sidebar-nav-item " href="/about/">About</a>
      <a class="sidebar-nav-item " href="/members/">Members</a>
      <a class="sidebar-nav-item " href="/research/">Research</a>
      <a class="sidebar-nav-item " href="/publications/">Publications</a>
      <a class="sidebar-nav-item " href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item active" href="/blog/">Blog</a>
      <a class="sidebar-nav-item " href="/contacts/">Contacts</a>
      <a class="sidebar-nav-item " href="/tag/">Tags</a>
    </nav>
    <p>&copy; Pilwon Hur.</p>
  </div>
</div>
<div class="content container">

<!-- Content appended here -->
<div class="franklin-content">
<h1 id="202485_crash_course_on_decision_process_i_mdp"><a href="#202485_crash_course_on_decision_process_i_mdp" class="header-anchor">&#91;2024/8/5&#93; Crash Course on Decision Process &#40;I&#41;: MDP</a></h1>
<p>ì‘ì„±ì: í—ˆí•„ì› with chatGPT</p>
<div class="container-100"><img src="image01.png" alt="" /></div>
<p>ì§€ë‚œ 2024ë…„ 8ì›” 5ì¼, ì—°êµ¬ì‹¤ í•™ìƒë“¤ê³¼ í•¨ê»˜ Markov Decision Process &#40;MDP&#41;ì— ëŒ€í•œ Crash Courseë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì´ ê°•ì˜ëŠ” ì£¼ë¡œ ë¡œë´‡ ì œì–´, Human-Robot Interaction, ê·¸ë¦¬ê³  ì›¨ì–´ëŸ¬ë¸” ë¡œë´‡ ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ MDP ê°œë…ì„ ì†Œê°œí•˜ëŠ” ë° ì¤‘ì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤. ì—°êµ¬ì‹¤ì˜ ë§ì€ í•™ìƒë“¤ì´ ì•„ì§ MDPì— ëŒ€í•œ ê°œë…ì´ ìµìˆ™í•˜ì§€ ì•Šì€ ê²ƒì„ ë³´ì•„ ì´ëŸ¬í•œ êµìœ¡ì´ í•„ìš”í•˜ë‹¤ê³  ëŠê¼ˆìŠµë‹ˆë‹¤. í•™ìƒë“¤ì€ ëŒ€ëµ 15ëª…ì´ ì°¸ì„í•˜ì˜€ìœ¼ë©°, ê°•ì˜ëŠ” ì˜ì–´ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë²ˆ ë¸”ë¡œê·¸ ê¸€ì—ì„œëŠ” ê°•ì˜ì˜ ì£¼ìš” ë‚´ìš©ì„ ê°„ëµíˆ ì •ë¦¬í•´ë³´ê² ìŠµë‹ˆë‹¤.</p>
<p>ì°¸ê³ ë¡œ, ê°•ì˜ìë£Œì™€ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì•„ë˜ì˜ github ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <a href="https://github.com/pilwonhur/hurgroup_cc_mdp">https://github.com/pilwonhur/hurgroup<em>cc</em>mdp</a></p>
<ol>
<li><p>MDPë€ ë¬´ì—‡ì¸ê°€?</p>
</li>
</ol>
<p>Markov Decision ProcessëŠ” ì˜ì‚¬ê²°ì • ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ì£¼ë¡œ ë¡œë´‡ ì œì–´ì™€ ê°™ì€ ì—°ì†ì ì¸ ì˜ì‚¬ê²°ì •ì´ í•„ìš”í•œ ë¶„ì•¼ì—ì„œ ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. MDPëŠ” ìƒíƒœ&#40;state&#41;, í–‰ë™&#40;action&#41;, ìƒíƒœ ì „ì´ í™•ë¥ &#40;transition probability&#41;, ê·¸ë¦¬ê³  ë³´ìƒ&#40;reward&#41;ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ MDPë¥¼ ì´í•´í•˜ëŠ” ì²« ê±¸ìŒì…ë‹ˆë‹¤.</p>
<ol start="2">
<li><p>Markov ì—°ì‡„ì™€ MDPì˜ ê´€ê³„</p>
</li>
</ol>
<p>MDPë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € Markov ì—°ì‡„&#40;Markov Chain&#41;ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. Markov ì—°ì‡„ëŠ” íŠ¹ì • ìƒíƒœì—ì„œ ë‹¤ë¥¸ ìƒíƒœë¡œ ì „ì´ë  í™•ë¥ ì„ ì •ì˜í•˜ëŠ” ëª¨ë¸ë¡œ, ì£¼ì–´ì§„ í˜„ì¬ ìƒíƒœë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ìƒíƒœë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤. ì´ëŠ” MDPì—ì„œë„ ë™ì¼í•˜ê²Œ ì ìš©ë˜ë©°, íŠ¹íˆ ìƒíƒœ ì „ì´ í™•ë¥ ì„ ë‹¤ë£° ë•Œ ì¤‘ìš”í•œ ê°œë…ì…ë‹ˆë‹¤.</p>
<ol start="3">
<li><p>ìƒíƒœ ì „ì´ì™€ í™•ë¥ </p>
</li>
</ol>
<p>ê°•ì˜ì—ì„œëŠ” ìƒíƒœ ì „ì´ í™•ë¥ ì„ ì´í•´í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ ë‚ ì”¨ ì˜ˆì¸¡ ëª¨ë¸ì„ ì˜ˆì‹œë¡œ ë“¤ì—ˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§‘ìœ¼ë©´ ë‚´ì¼ë„ ë§‘ì„ í™•ë¥ , í˜¹ì€ ë¹„ê°€ ì˜¬ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í•™ìƒë“¤ì´ í™•ë¥  ê¸°ë°˜ì˜ ìƒíƒœ ì „ì´ ê°œë…ì„ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>
<ol start="4">
<li><p>ë³´ìƒê³¼ ê°€ì¹˜ í•¨ìˆ˜</p>
</li>
</ol>
<p>MDPì—ì„œ ë³´ìƒ&#40;reward&#41;ì€ íŠ¹ì • í–‰ë™ì„ ì·¨í–ˆì„ ë•Œ ì–»ì„ ìˆ˜ ìˆëŠ” ì´ìµì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ë³´ìƒì„ í†µí•´ ìµœì ì˜ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ MDPì˜ ëª©í‘œì…ë‹ˆë‹¤. ë˜í•œ, í• ì¸ìœ¨&#40;discount factor&#41;ì„ ì ìš©í•˜ì—¬ ë¯¸ë˜ì˜ ë³´ìƒì„ í˜„ì¬ ê°€ì¹˜ë¡œ í™˜ì‚°í•˜ëŠ” ë°©ë²•ë„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ë¡œë´‡ì´ í˜„ì¬ì˜ í–‰ë™ì´ ë¯¸ë˜ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹ ì§€ ê³ ë ¤í•˜ê²Œ í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤.</p>
<ol start="5">
<li><p>Bellman ë°©ì •ì‹</p>
</li>
</ol>
<p>Bellman ë°©ì •ì‹ì€ MDPì˜ í•µì‹¬ ê°œë… ì¤‘ í•˜ë‚˜ë¡œ, ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜&#40;state value function&#41;ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í˜„ì¬ ìƒíƒœì—ì„œ ìµœì ì˜ í–‰ë™ì„ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì •ì±…&#40;policy&#41;ì„ ì°¾ëŠ” ê³¼ì •ì´ ì„¤ëª…ë˜ì—ˆìŠµë‹ˆë‹¤. í•™ìƒë“¤ì´ Bellman ë°©ì •ì‹ì„ ì´í•´í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì—¬ëŸ¬ ì˜ˆì œë¥¼ í†µí•´ ì ì§„ì ìœ¼ë¡œ ê°œë…ì„ ì†Œê°œí–ˆìŠµë‹ˆë‹¤.</p>
<ol start="6">
<li><p>ì •ì±… ë°˜ë³µê³¼ ê°€ì¹˜ ë°˜ë³µ</p>
</li>
</ol>
<p>MDP ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì •ì±… ë°˜ë³µ&#40;policy iteration&#41;ê³¼ ê°€ì¹˜ ë°˜ë³µ&#40;value iteration&#41;ì´ë¼ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë°©ë²•ì´ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. ì •ì±… ë°˜ë³µì€ í˜„ì¬ì˜ ì •ì±…ì„ í‰ê°€í•˜ê³  ê°œì„ í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ëŠ” ë°©ë²•ì´ë©°, ê°€ì¹˜ ë°˜ë³µì€ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ í†µí•´ ìµœì ì˜ ì •ì±…ì„ ì°¾ì•„ê°€ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë‘ ê°€ì§€ ë°©ë²•ì„ í†µí•´ MDP ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ í•™ìƒë“¤ê³¼ í•¨ê»˜ ë…¼ì˜í–ˆìŠµë‹ˆë‹¤.</p>
<ol start="7">
<li><p>ë¡œë´‡ ì œì–´ì—ì˜ ì‘ìš©</p>
</li>
</ol>
<p>ë§ˆì§€ë§‰ìœ¼ë¡œ, MDPë¥¼ ë¡œë´‡ ì œì–´ì— ì ìš©í•˜ëŠ” ì˜ˆì‹œë¡œ íœë“ˆëŸ¼ ìš´ë™ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ë¡œë´‡ì´ íœë“ˆëŸ¼ì„ íŠ¹ì • ìœ„ì¹˜ì— ìœ ì§€í•˜ê±°ë‚˜ ì›í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ê²Œ í•˜ê¸° ìœ„í•´ ì–´ë–»ê²Œ MDPë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ í•™ìƒë“¤ì€ ì´ë¡ ì ì¸ ê°œë…ì´ ì‹¤ì œ ë¡œë´‡ ì œì–´ ë¬¸ì œì— ì–´ë–»ê²Œ ì ìš©ë˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>
<p>ì´ë²ˆ ê°•ì˜ë¥¼ í†µí•´ í•™ìƒë“¤ì´ MDPì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ë¡œë´‡ ì œì–´ì™€ ê°™ì€ ì‹¤ì œ ë¬¸ì œì— ì ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ë°˜ì„ ë§ˆë ¨í•  ìˆ˜ ìˆê¸°ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤. ë‹¤ìŒ Crash Courseì—ì„œëŠ” ê°•í™”í•™ìŠµ&#40;Reinforcement Learning&#41;ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìœ¼ë©°, ì¶”í›„ POMDP&#40;Partially Observable Markov Decision Process&#41;ë„ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ë” ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ í•  ìˆ˜ ìˆë„ë¡ ì§€ì†ì ì¸ êµìœ¡ì„ ì œê³µí•  ê³„íšì…ë‹ˆë‹¤.</p>
<p>ì•„ë˜ëŠ” ì‹¤ì œ ê°•ì˜ ì˜ìƒì˜ ë‚´ìš© ìš”ì•½ì…ë‹ˆë‹¤. <a href="https://youtu.be/UD_jb2W8jCA"><img src="https://img.youtube.com/vi/UD_jb2W8jCA/hqdefault.jpg" alt="" /></a></p>
<p>00:00:00 ğŸ‘‹ Introduction and Participant Details</p>
<p>00:03:27 ğŸ“ Introduction to Markov Decision Processes &#40;MDP&#41;</p>
<p>00:08:23 ğŸ§  Introduction to Markov Decision Processes and Related Concepts</p>
<p>00:12:22 ğŸš‚ Understanding Markov Chains and Processes</p>
<p>00:15:52 ğŸ“Š Understanding Markov Chains</p>
<p>00:20:45 ğŸ“Š Understanding State Transitions in Markov Decision Processes &#40;MDPs&#41;</p>
<p>00:25:30 ğŸŒŸ Understanding State Transitions in Markov Chains</p>
<p>00:28:41 ğŸŒ¦ï¸ Markov Chains and Weather Prediction</p>
<p>00:31:05 ğŸ² Understanding Random Inputs and Probabilities in Systems</p>
<p>00:34:53 ğŸ¯ Understanding State Transitions and Distributions</p>
<p>00:39:25 ğŸ¯ Understanding Transition Probabilities in MDP</p>
<p>00:42:40 ğŸ” Understanding Recurrent Property and Transition to MDP</p>
<p>00:47:23 ğŸ” Discussion on State Estimation and MDP</p>
<p>00:50:00 âš™ï¸ Transition Matrix and Noise Management in MDP</p>
<p>00:52:38 ğŸ” Detailing Markov Decision Processes &#40;MDP&#41;</p>
<p>00:56:29 ğŸ® Components and Goals of Markov Decision Processes &#40;MDP&#41;</p>
<p>01:00:57 ğŸŒŸ Understanding States and Actions in MDP</p>
<p>01:04:41 ğŸ“Œ Components of Markov Decision Processes &#40;MDP&#41;</p>
<p>01:08:32 ğŸ“ Understanding States and Actions in MDP</p>
<p>01:12:56 ğŸ² Adding Stochasticity and Policy in MDP</p>
<p>01:15:34 ğŸ” Understanding State Transition Probabilities in MDP</p>
<p>01:18:40 ğŸŒŸ Understanding State Transition and Reward in MDP</p>
<p>01:22:35 âš¡ Understanding Rewards in Markov Decision Processes</p>
<p>01:26:48 ğŸ“Š Understanding Rewards and Discount Factors in MDP</p>
<p>01:29:20 ğŸ’¡ Understanding Value Functions and Discount Factors</p>
<p>01:32:51 ğŸ” Understanding Discount Factors in MDP</p>
<p>01:35:17 ğŸ“˜ Understanding the Discount Rate in MDP</p>
<p>01:38:15 ğŸ“Š Understanding Expected Return in MDP</p>
<p>01:40:27 ğŸš€ Understanding the Value Function in MDP</p>
<p>01:44:40 ğŸ¯ Understanding Policies in MDP</p>
<p>01:48:51 ğŸš€ Insights on MDP Policies and Implementation</p>
<p>01:51:43 ğŸ’¡ Understanding the Value Function in MDP</p>
<p>01:59:06 ğŸ“˜ Understanding the Bellman Equation</p>
<p>02:02:15 ğŸ”„ Exploring the Value Function in MDPs</p>
<p>02:05:13 ğŸ” Key Aspects of the Bellman Equation and MDP</p>
<p>02:10:06 ğŸ§  Understanding the Bellman Expectation Equation</p>
<p>02:13:35 âœ… Understanding the Bellman Optimality Equation</p>
<p>02:16:18 ğŸ” Understanding the Bellman Equation and Optimal Policies</p>
<p>02:19:15 ğŸš€ Key Methods to Solve MDP Problems</p>
<p>02:23:54 ğŸ¯ Key Points on Policy Gradient Methods and Breaks</p>
<p>02:27:25 ğŸ“˜ Understanding the Bellman Equation and Dynamic Programming</p>
<p>02:31:49 ğŸ” Understanding Iterative Approximation and Convergence in MDP</p>
<p>02:36:14 ğŸš€ Introduction to Advanced Mathematical Concepts</p>
<p>02:39:14 ğŸ“‰ Convergence and Divergence in Processes</p>
<p>02:41:33 ğŸ” Convergence of Value Functions and the Bellman Equation</p>
<p>02:45:36 ğŸš€ Understanding Policy Iteration and Bellman Equations</p>
<p>02:48:37 ğŸš€ Understanding Policy Iteration in MDP</p>
<p>02:51:57 ğŸ”„ Policy Improvement and Value Function in MDP</p>
<p>02:56:10 ğŸ“Š Approximate Policy Evaluation and Improvement</p>
<p>02:58:48 ğŸ¯ Understanding Q Function and Value Iteration</p>
<p>03:03:46 ğŸ” Policy Iteration in Markov Decision Processes</p>
<p>03:08:39 ğŸ§  Understanding Policy Evaluation and Improvement</p>
<p>03:12:39 ğŸ“Š Evaluating and Improving Policies in MDP</p>
<p>03:14:53 ğŸ§  Optimal Value Function and Action Selection</p>
<p>03:17:41 ğŸš€ Analyzing State Transitions and Values in MDP</p>
<p>03:20:56 ğŸ“ˆ Key Concepts in Markov Decision Processes &#40;MDP&#41;</p>
<p>03:25:55 ğŸ’¡ Understanding Markov Decision Processes</p>
<p>03:30:00 ğŸš€ Value Iteration and Policy Iteration Challenges</p>
<p>03:32:23 ğŸš€ Applying MDP to Pendulum Motion</p>
<p>03:36:18 ğŸ¯ Key Concepts in MDP and Reinforcement Learning</p>
<p>03:40:20 âœ¨ Understanding State Transitions and Costs</p>
<p>03:41:50 ğŸš€ Understanding Challenges in Applying MDP to Robotics</p>
<p>03:47:22 âœ¨ Discussion on Initial State and Dynamic Programming</p>
<div class="page-foot">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Pilwon Hur. Last modified: October 08, 2024.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
    
  </body>
</html>
