<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/favicon.png">

   <title>[2024/8/21] Crash Course on Decision Process (II): Reinforcement Learning</title>  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1><a href="/">HUR Group</a></h1>
      <p class="lead">Dynamics and Biomechatronics Lab</p>
    </div>
    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">Home</a>
      <a class="sidebar-nav-item " href="/about/">About</a>
      <a class="sidebar-nav-item " href="/members/">Members</a>
      <a class="sidebar-nav-item " href="/research/">Research</a>
      <a class="sidebar-nav-item " href="/publications/">Publications</a>
      <a class="sidebar-nav-item " href="/teaching/">Teaching</a>
      <a class="sidebar-nav-item active" href="/blog/">Blog</a>
      <a class="sidebar-nav-item " href="/contacts/">Contacts</a>
      <a class="sidebar-nav-item " href="/tag/">Tags</a>
    </nav>
    <p>&copy; Pilwon Hur.</p>
  </div>
</div>
<div class="content container">

<!-- Content appended here -->
<div class="franklin-content">
<h1 id="2024821_crash_course_on_decision_process_ii_reinforcement_learning"><a href="#2024821_crash_course_on_decision_process_ii_reinforcement_learning" class="header-anchor">&#91;2024/8/21&#93; Crash Course on Decision Process &#40;II&#41;: Reinforcement Learning</a></h1>
<p>ì‘ì„±ì: í—ˆí•„ì› with chatGPT</p>
<div class="container-100"><img src="image01.png" alt="" /></div>
<p>ì§€ë‚œ 2024ë…„ 8ì›” 21ì¼, ì—°êµ¬ì‹¤ í•™ìƒë“¤ê³¼ í•¨ê»˜ Reinforcement Learning &#40;ê°•í™”í•™ìŠµ&#41;ì— ëŒ€í•œ Crash Courseë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ê°•í™”í•™ìŠµì€ MDP&#40;Markov Decision Process&#41;ë¥¼ í™•ì¥í•œ ê°œë…ìœ¼ë¡œ, ë¡œë´‡ ì œì–´, Human-Robot Interaction, ê·¸ë¦¬ê³  ì›¨ì–´ëŸ¬ë¸” ë¡œë´‡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‘ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ê°•ì˜ì—ëŠ” ì•½ 15ëª…ì˜ í•™ìƒë“¤ì´ ì°¸ì„í•˜ì˜€ìœ¼ë©°, ê°•ì˜ëŠ” ì˜ì–´ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë²ˆ ë¸”ë¡œê·¸ ê¸€ì—ì„œëŠ” ê°•í™”í•™ìŠµì˜ ì£¼ìš” ê°œë…ê³¼ ê°•ì˜ ë‚´ìš©ì„ ê°„ëµíˆ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.</p>
<p>ì°¸ê³ ë¡œ, ê°•ì˜ìë£Œì™€ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì•„ë˜ì˜ github ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <a href="https://github.com/pilwonhur/hurgroup_cc_mdp">https://github.com/pilwonhur/hurgroup<em>cc</em>mdp</a></p>
<ol>
<li><p>Reinforcement Learningì˜ ê°œìš”</p>
</li>
</ol>
<p>ê°•í™”í•™ìŠµì€ ì—ì´ì „íŠ¸&#40;agent&#41;ê°€ í™˜ê²½&#40;environment&#41;ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©´ì„œ ìµœì ì˜ í–‰ë™ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ì£¼ì–´ì§„ ìƒíƒœ&#40;state&#41;ì—ì„œ í–‰ë™&#40;action&#41;ì„ ì„ íƒí•˜ê³ , ê·¸ ê²°ê³¼ë¡œ ë³´ìƒ&#40;reward&#41;ì„ ë°›ìœ¼ë©° í•™ìŠµí•©ë‹ˆë‹¤. MDPì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê°•í™”í•™ìŠµì—ì„œë„ ìƒíƒœ, í–‰ë™, ìƒíƒœ ì „ì´ í™•ë¥ , ê·¸ë¦¬ê³  ë³´ìƒì˜ ê°œë…ì´ ì¤‘ìš”í•˜ì§€ë§Œ, ê°•í™”í•™ìŠµì—ì„œëŠ” íŠ¹íˆ ì—ì´ì „íŠ¸ê°€ ê²½í—˜ì„ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•´ ë‚˜ê°„ë‹¤ëŠ” ì ì—ì„œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.</p>
<ol start="2">
<li><p>MDPì—ì„œ RLë¡œì˜ í™•ì¥</p>
</li>
</ol>
<p>MDPëŠ” ê°•í™”í•™ìŠµì˜ ê¸°ë³¸ì ì¸ í‹€ì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ MDPì™€ ë‹¬ë¦¬ ê°•í™”í•™ìŠµì—ì„œëŠ” ì—ì´ì „íŠ¸ê°€ í™˜ê²½ì˜ ë™ì ì¸ íŠ¹ì„±ì„ í•™ìŠµí•˜ê³  ì ì‘í•´ì•¼ í•©ë‹ˆë‹¤. ê°•ì˜ì—ì„œëŠ” MDPì™€ RLì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ë©°, íŠ¹íˆ ìƒíƒœ ì „ì´ í™•ë¥ ì„ ì •í™•íˆ ì•Œì§€ ëª»í•˜ëŠ” ìƒí™©ì—ì„œ ì—ì´ì „íŠ¸ê°€ ì–´ë–»ê²Œ ìµœì ì˜ ì •ì±…&#40;policy&#41;ì„ ì°¾ì•„ë‚˜ê°€ëŠ”ì§€ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì ì—ì„œ ê°•í™”í•™ìŠµì€ íƒìƒ‰&#40;exploration&#41;ê³¼ í™œìš©&#40;exploitation&#41; ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê³¼ì œê°€ ë©ë‹ˆë‹¤.</p>
<ol start="3">
<li><p>íƒìƒ‰ê³¼ í™œìš©ì˜ ê· í˜•</p>
</li>
</ol>
<p>ê°•í™”í•™ìŠµì—ì„œ ì—ì´ì „íŠ¸ëŠ” ìƒˆë¡œìš´ ìƒíƒœì™€ í–‰ë™ì„ íƒìƒ‰í•˜ë©´ì„œë„, í˜„ì¬ê¹Œì§€ í•™ìŠµí•œ ì •ì±…ì„ í™œìš©í•˜ì—¬ ë³´ìƒì„ ìµœëŒ€í™”í•´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œ â€œepsilon-greedyâ€ ì •ì±…ì´ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì¼ì • í™•ë¥ ë¡œ ìƒˆë¡œìš´ í–‰ë™ì„ íƒìƒ‰í•˜ê³ , ë‚˜ë¨¸ì§€ í™•ë¥ ë¡œëŠ” í˜„ì¬ì˜ ìµœì  ì •ì±…ì„ ë”°ë¥´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê°•ì˜ì—ì„œëŠ” ì´ëŸ¬í•œ íƒìƒ‰ê³¼ í™œìš©ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ì—¬ëŸ¬ ì „ëµë“¤ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤.</p>
<ol start="4">
<li><p>ê°€ì¹˜ í•¨ìˆ˜ì™€ ì •ì±…</p>
</li>
</ol>
<p>ê°•ì˜ì—ì„œëŠ” ê°•í™”í•™ìŠµì—ì„œ ì¤‘ìš”í•œ ê°€ì¹˜ í•¨ìˆ˜&#40;value function&#41;ì™€ í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜&#40;action value function&#41;ì— ëŒ€í•´ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ì´ í•¨ìˆ˜ë“¤ì€ ì—ì´ì „íŠ¸ê°€ íŠ¹ì • ìƒíƒœì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ë³´ìƒì˜ ê¸°ëŒ€ê°’ì„ ë‚˜íƒ€ë‚´ë©°, ìµœì ì˜ í–‰ë™ì„ ì„ íƒí•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. íŠ¹íˆ Bellman ë°©ì •ì‹ì„ í†µí•´ ê°€ì¹˜ í•¨ìˆ˜ë¥¼ ê°±ì‹ í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì  ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤.</p>
<ol start="5">
<li><p>ì£¼ìš” ì•Œê³ ë¦¬ì¦˜</p>
</li>
</ol>
<p>ê°•ì˜ í›„ë°˜ë¶€ì—ì„œëŠ” ê°•í™”í•™ìŠµì—ì„œ ì‚¬ìš©ë˜ëŠ” ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œëŠ” SARSAì™€ Q-Learningì´ ìˆìœ¼ë©°, ì´ ë‘ ì•Œê³ ë¦¬ì¦˜ì˜ ì°¨ì´ì ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ê°•í™”í•™ìŠµì„ ê¹Šì´ ìˆê²Œ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤. SARSAëŠ” í˜„ì¬ ì •ì±…ì„ ë”°ë¥´ëŠ” â€œOn-Policyâ€ ë°©ë²•ì´ê³ , Q-Learningì€ ìµœì ì˜ ì •ì±…ì„ ì°¾ëŠ” â€œOff-Policyâ€ ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë‘ ê°€ì§€ ë°©ë²•ì„ í†µí•´ ì—ì´ì „íŠ¸ê°€ ì–´ë–»ê²Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ ì˜ˆì‹œì™€ í•¨ê»˜ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.</p>
<ol start="6">
<li><p>ì‹¬ì¸µ ê°•í™”í•™ìŠµ&#40;Deep Reinforcement Learning&#41;</p>
</li>
</ol>
<p>ìµœê·¼ ê°•í™”í•™ìŠµì—ì„œ ê°€ì¥ ì£¼ëª©ë°›ê³  ìˆëŠ” ë¶„ì•¼ ì¤‘ í•˜ë‚˜ëŠ” ì‹¬ì¸µ ê°•í™”í•™ìŠµì…ë‹ˆë‹¤. ì‹¬ì¸µ ê°•í™”í•™ìŠµì€ ì‹ ê²½ë§&#40;neural network&#41;ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ í™˜ê²½ì—ì„œë„ ì—ì´ì „íŠ¸ê°€ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ê°•ì˜ì—ì„œëŠ” Deep Q-Network&#40;DQN&#41;ì„ í¬í•¨í•œ ì—¬ëŸ¬ ì‹¬ì¸µ ê°•í™”í•™ìŠµ ê¸°ë²•ë“¤ì„ ì†Œê°œí•˜ë©°, ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì´ ê¸°ì¡´ ê°•í™”í•™ìŠµì˜ í•œê³„ë¥¼ ì–´ë–»ê²Œ ê·¹ë³µí•˜ëŠ”ì§€ì— ëŒ€í•´ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ê²½í—˜ ì¬ìƒ&#40;experience replay&#41;ê³¼ íƒ€ê¹ƒ ë„¤íŠ¸ì›Œí¬&#40;target network&#41;ì™€ ê°™ì€ ê¸°ë²•ë“¤ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì˜ ì•ˆì •ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤.</p>
<ol start="7">
<li><p>ì •ì±… ê²½ì‚¬ë²•ê³¼ ì•¡í„°-í¬ë¦¬í‹±</p>
</li>
</ol>
<p>ë§ˆì§€ë§‰ìœ¼ë¡œ, ì •ì±… ê²½ì‚¬ë²•&#40;policy gradient&#41;ê³¼ ì•¡í„°-í¬ë¦¬í‹±&#40;actor-critic&#41; ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤. ì •ì±… ê²½ì‚¬ë²•ì€ ì—ì´ì „íŠ¸ê°€ ì§ì ‘ ìµœì ì˜ ì •ì±…ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ë³µì¡í•œ í™˜ê²½ì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì•¡í„°-í¬ë¦¬í‹± ì•Œê³ ë¦¬ì¦˜ì€ ì •ì±…&#40;actor&#41;ê³¼ ê°€ì¹˜ í•¨ìˆ˜&#40;critic&#41;ë¥¼ í•¨ê»˜ í•™ìŠµí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ê°•í™”í•™ìŠµì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ë³µì¡í•œ ë¡œë´‡ ì œì–´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ê°•ë ¥í•œ ë„êµ¬ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
<p>ì´ë²ˆ ê°•ì˜ë¥¼ í†µí•´ í•™ìƒë“¤ì´ ê°•í™”í•™ìŠµì˜ ê¸°ë³¸ ê°œë…ë¶€í„° ì‹¬í™”ëœ ì•Œê³ ë¦¬ì¦˜ê¹Œì§€ ì´í•´í•  ìˆ˜ ìˆëŠ” ê¸°íšŒë¥¼ ê°€ì¡Œê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ë‹¤ìŒ Crash Courseì—ì„œëŠ” POMDP&#40;Partially Observable Markov Decision Process&#41;ì— ëŒ€í•´ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤. í•™ìƒë“¤ì´ ë”ìš± ê¹Šì´ ìˆëŠ” í•™ìŠµì„ í•  ìˆ˜ ìˆë„ë¡ ì§€ì†ì ì¸ êµìœ¡ì„ ì œê³µí•  ê³„íšì…ë‹ˆë‹¤.</p>
<p>ì•„ë˜ëŠ” ì‹¤ì œ ê°•ì˜ ì˜ìƒì˜ ë‚´ìš© ìš”ì•½ì…ë‹ˆë‹¤. <a href="https://youtu.be/q-_fvkRdQE0"><img src="https://img.youtube.com/vi/q-_fvkRdQE0/hqdefault.jpg" alt="" /></a></p>
<p>00:00:00 ğŸ“š Lecture Preparation and Challenges</p>
<p>00:04:52 ğŸ“š Overview of Reinforcement Learning and Examples</p>
<p>00:08:35 ğŸ“˜ Overview of Markov Decision Processes &#40;MDP&#41;</p>
<p>00:10:08 ğŸ§  Understanding Transition Probability in MDP vs. RL</p>
<p>00:11:40 ğŸ¤– Understanding Agents and Environments in RL</p>
<p>00:13:39 ğŸ¤– Understanding Markov Decision Processes and Reinforcement Learning</p>
<p>00:16:33 â™»ï¸ Understanding the Bellman Equation in MDP</p>
<p>00:18:10 âš™ï¸ Dynamic Programming in MDP</p>
<p>00:19:37 âš™ï¸ Transition from MDP to RL</p>
<p>00:20:56 ğŸ¤– Challenges in Modeling and Using MDP</p>
<p>00:22:40 ğŸ” Challenges in Solving High Degree of Freedom Problems</p>
<p>00:24:25 ğŸŒ Balancing Exploration and Exploitation in Reinforcement Learning</p>
<p>00:26:01 ğŸŒ€ Reinforcement Learning vs. Markov Decision Processes</p>
<p>00:29:05 ğŸ¤¸â€â™‚ï¸ Understanding Energy Gain in Motion and MDP Application</p>
<p>00:31:21 ğŸŒ€ Understanding Phase Portraits and Energy in Motion</p>
<p>00:35:13 ğŸŒ€ Understanding Angular Dynamics in Programming</p>
<p>00:37:10 ğŸ¯ Pendulum Control with RL and MDP</p>
<p>00:41:50 ğŸ” Understanding Deterministic Models in MDP</p>
<p>00:45:29 ğŸ” Understanding Uncertainty in Systems</p>
<p>00:47:55 âš™ï¸ Understanding State Changes in Discretization</p>
<p>00:49:48 ğŸ” Understanding State Transitions and Transition Probability Functions</p>
<p>00:53:45 ğŸ” Understanding State Transition Probability in RL</p>
<p>00:56:21 ğŸ” Key Concepts in Grid and Delta T Adjustments</p>
<p>00:59:43 ğŸ¤” Challenges and Considerations in MDP</p>
<p>01:02:57 âš™ï¸ Strategies in Reinforcement Learning for Optimal Control</p>
<p>01:07:21 ğŸ½ï¸ Cooking with Repeated Ingredients</p>
<p>01:10:56 ğŸ¤– Challenges and Solutions in Reinforcement Learning for Robotics</p>
<p>01:16:04 ğŸŒŸ Reinforcement Learning in Healthcare and Basic Concepts</p>
<p>01:17:54 ğŸ¤– Key Concepts of Reinforcement Learning</p>
<p>01:19:08 ğŸ“˜ Importance of Value and Action Value Functions in RL</p>
<p>01:21:55 ğŸ” Understanding MDP and Reinforcement Learning</p>
<p>01:23:44 ğŸŒŸ Key Concepts in Exploration and Exploitation</p>
<p>01:25:05 ğŸ¯ Balancing Exploration and Exploitation in Reinforcement Learning</p>
<p>01:28:05 ğŸ” Key Algorithms in Reinforcement Learning</p>
<p>01:30:29 ğŸ¯ Key Methods in Reinforcement Learning</p>
<p>01:32:18 ğŸ“˜ Temporal Difference Learning in Reinforcement Learning</p>
<p>01:33:37 ğŸ” Understanding Temporal Difference Learning in RL</p>
<p>01:35:09 ğŸ” Key Differences Between On-Policy and Off-Policy in Reinforcement Learning</p>
<p>01:36:51 ğŸ” Understanding SARSA and Q-Learning</p>
<p>01:38:28 ğŸš€ Key Concepts in Reinforcement Learning Algorithms</p>
<p>01:42:18 ğŸ” Estimating Areas and Value Functions</p>
<p>01:45:27 ğŸ¯ Understanding Episode Returns in Reinforcement Learning</p>
<p>01:47:50 ğŸ” Understanding the Update Equation in RL</p>
<p>01:49:07 ğŸ” Understanding the Update Equation in Reinforcement Learning</p>
<p>01:50:47 ğŸ§  Understanding the Update Rule and Convergence in RL</p>
<p>01:52:55 ğŸš€ Importance of Exploration in RL</p>
<p>01:54:31 ğŸ”„ Temporal Difference in Reinforcement Learning</p>
<p>01:56:21 ğŸ¥¾ Understanding Bootstrapping in Computer Science</p>
<p>02:00:46 ğŸ” Bootstrapping and Temporal Difference in Reinforcement Learning</p>
<p>02:04:14 ğŸ“š Understanding Deep Reinforcement Learning and Temporal Difference</p>
<p>02:08:46 ğŸ“Œ Project Timeline and Testing Strategy</p>
<p>02:11:10 ğŸ“ Discussion on X and V in Matrix Operations</p>
<p>02:16:49 ğŸ“š Introduction to Algorithms in Reinforcement Learning</p>
<p>02:19:14 ğŸ”„ Generalized Policy Iteration in Reinforcement Learning</p>
<p>02:20:37 ğŸ¯ Understanding Temporal Difference Update in RL</p>
<p>02:22:50 ğŸ¤– Understanding Greedy and Epsilon-Greedy Policies</p>
<p>02:25:17 ğŸ¯ Action Value vs. State Value in Reinforcement Learning</p>
<p>02:26:54 ğŸ¯ Epsilon Greedy Policy in Reinforcement Learning</p>
<p>02:29:29 ğŸ¯ Understanding Sarsa in Reinforcement Learning</p>
<p>02:32:12 ğŸ¯ Understanding the Action and Reward Process in Reinforcement Learning</p>
<p>02:34:42 âš™ï¸ Understanding Q-Learning and Convergence</p>
<p>02:36:31 ğŸš€ Q-Learning vs. Exploration Challenges</p>
<p>02:39:38 ğŸ’¡ Differences Between Salsa and Q-Learning</p>
<p>02:43:55 ğŸ” Key Concepts in Q-learning</p>
<p>02:46:32 ğŸš€ Scalability and Combining Deep Learning with Reinforcement Learning</p>
<p>02:47:50 ğŸš€ Using Deep Learning in Q-Learning</p>
<p>02:50:20 ğŸš€ Importance of Deep Learning in Reinforcement Learning</p>
<p>02:51:50 ğŸ“Š End-to-End Learning and Deep Learning Basics</p>
<p>02:52:58 ğŸ¤– Neural Network Layers and Activation Functions</p>
<p>02:54:41 âš™ï¸ Understanding Backpropagation and Gradient Descent</p>
<p>02:56:20 ğŸ” Effects of Small Slopes in Activation Functions</p>
<p>02:57:38 ğŸ” Understanding Loss Functions and Optimization in RL</p>
<p>03:02:14 ğŸ“Š Understanding Predicted and True Values in Networks</p>
<p>03:04:40 ğŸš€ Implementing Reinforcement Learning with Python Libraries</p>
<p>03:07:49 ğŸ± Lunch Break and Preferences</p>
<p>03:13:39 ğŸ” Understanding Q Function and Training in Reinforcement Learning</p>
<p>03:16:57 âš™ï¸ Q-Learning Update and Epsilon-Greedy Strategy</p>
<p>03:19:23 ğŸš€ Key Concepts in Q-function and Temporal Difference</p>
<p>03:22:02 âš™ï¸ Understanding Deep Q-Networks &#40;DQN&#41;</p>
<p>03:26:31 ğŸ“š Understanding Experience Replay and Target Networks</p>
<p>03:30:43 ğŸ¯ Stability in Reinforcement Learning</p>
<p>03:33:36 ğŸ“š Reinforcement Learning Models and Methods</p>
<p>03:37:36 ğŸš€ Function Approximators and Network Updates in RL</p>
<p>03:42:15 ğŸ“Š Reinforcement Learning Concepts: Policy Gradient and Value Functions</p>
<p>03:44:16 ğŸ“Š Understanding the Gradient Function in Reinforcement Learning</p>
<p>03:46:23 ğŸ” Understanding Policy Gradient Method and Reinforce Algorithm</p>
<p>03:48:32 ğŸ¯ Actor-Critic Algorithms in Reinforcement Learning</p>
<p>03:50:30 ğŸ¯ Understanding Policy Gradient and Reinforce Algorithm</p>
<p>03:51:51 ğŸ¯ Actor-Critic Method in Reinforcement Learning</p>
<p>03:55:15 ğŸ¯ Understanding Policy Networks and Cross-Entropy</p>
<p>03:57:15 ğŸ­ Actor-Critic and Advantage Functions</p>
<p>03:59:14 ğŸ” Overview of Reinforcement Learning Algorithms</p>
<p>04:01:06 ğŸ“ Hybrid Methods in Reinforcement Learning</p>
<p>04:02:22 ğŸ” Understanding Safe Reinforcement Learning and Its Methods</p>
<p>04:06:56 ğŸš€ Overview of Dynamic Grid World and Algorithms</p>
<p>04:09:52 ğŸ¤” Planning for Friday</p>
<div class="page-foot">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Pilwon Hur. Last modified: October 08, 2024.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    
    
  </body>
</html>
